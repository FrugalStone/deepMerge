{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aea0ae23-6dc8-4765-8d6b-21ddc0295655",
   "metadata": {},
   "source": [
    "# INFORMATION RETRIEVAL\n",
    "## Deep Merge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f3719c-7489-40f7-9e7f-a424d23eb1c8",
   "metadata": {},
   "source": [
    "- Abhirup Paul - CB.EN.U4CSE20401 <br>\n",
    "- Anjali Ganesan - CB.EN.U4CSE20612 <br>\n",
    "- Devaganga - CB.EN.U4CSE20415 <br>\n",
    "- Sasidharan GS - CB.EN.U4CSE20458 <br>\n",
    "- Shivaramakrishnan SS - CB.EN.U4CSE20460"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9900ec9-0587-4f3c-b74a-533a6ccda2b2",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7909e82-dbcf-4d22-a92e-2d5c28a2f6d8",
   "metadata": {},
   "source": [
    "## Loading & Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d72cd305-5ba0-4e02-a179-321864eb68a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1b07941-5cae-4c04-ba4e-820a95ac65cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"dancing little vulture\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c91891ac-f536-452c-85d1-0b8ab22d0577",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return file.read()\n",
    "\n",
    "file_paths = [\"control.txt\", \"imagination.txt\", \"rap_god.txt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77550702-a73d-4264-9932-4b0a3a3c4db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(document):\n",
    "    tokens = nltk.word_tokenize(document)\n",
    "    tokens = [word.lower() for word in tokens if word.isalnum()]\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "    return tokens\n",
    "\n",
    "preprocessed_documents = [preprocess(read_text_file(file_path)) \n",
    "                          for file_path in file_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05e7d65f-1bd4-4b22-b76a-8f01a1968edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {'little': [0, 0, 2, 2], 'vulture': [0, 0], 'come': [0, 0, 2, 2], 'nest': [0, 0], 'catch': [0, 0], 'world': [0, 0], 'impressed': [0, 0], 'built': [0, 0], 'culture': [0, 0], 'lies': [0, 0], 'fortress': [0, 0], 'fit': [0, 0], 'surprise': [0, 0], 'shit': [0, 2, 2], 'uncover': [0], 'sonnets': [0], 'sweeping': [0], 'heads': [0], 'move': [0], 'sand': [0], 'blocks': [0], 'roots': [0], 'wish': [0], 'spread': [0], 'need': [0, 2], 'control': [0, 1, 1], 'eyes': [0], 'every': [0, 2], 'corner': [0], 'nothing': [0, 1], 'could': [0, 2], 'know': [0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2], 'speck': [0], 'disorder': [0], 'buildings': [0, 0], 'glisten': [0, 0], 'around': [0, 0, 1, 1, 1, 2], 'cities': [0, 0], 'rising': [0, 0], 'ground': [0, 0], 'shape': [0], 'heart': [0], 'drag': [0], 'dark': [0], 'oh': [0, 0, 1, 1, 1, 1, 1, 1, 1, 2, 2], 'paranoia': [0, 0], 'led': [0, 0], 'detain': [0, 0], 'million': [0, 0, 2], 'getting': [0, 0], 'way': [0, 0, 2, 2, 2], 'feel': [0, 0, 2, 2, 2, 2, 2, 2], 'commerce': [0, 0], 'pulsing': [0, 0], 'veins': [0, 0], 'breaks': [0, 0, 0, 0], 'weight': [0, 0], 'war': [0, 0], 'said': [0, 2, 2, 2], 'held': [0], 'still': [0, 2, 2, 2, 2, 2, 2, 2], 'cave': [0, 0, 0], 'would': [1], 'try': [1, 1, 1, 1, 1, 2], 'throw': [1, 2], 'away': [1, 2], 'asked': [1, 2], 'say': [1, 2, 2, 2, 2, 2, 2, 2], 'careful': [1], 'love': [1, 1, 1], 'death': [1], 'leave': [1], 'want': [1, 2, 2], 'stay': [1], 'keep': [1, 2], 'waiting': [1, 2], 'ca': [1, 1, 1, 1, 1, 1, 2], 'change': [1, 1], 'things': [1, 1, 1, 1, 1], 'summer': [1, 1], 'somewhere': [1, 1, 2], 'cold': [1, 1], 'even': [1, 1, 2, 2], 'look': [1, 1, 2, 2], 'eye': [1, 1], 'tried': [1, 1, 2], 'chorus': [1, 1, 1], 'see': [1, 1, 1, 2], 'us': [1, 1, 1], 'dancing': [1, 1, 1], 'better': [1, 1, 1], 'one': [1, 1, 1, 2, 2, 2, 2, 2], 'yeah': [1, 1, 1, 1, 1, 1, 1, 2], 'imagination': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'verse': [1], '2': [1], 'left': [1], 'cry': [1], 'guess': [1], 'living': [1], 'drought': [1], 'raining': [1], 'like': [1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], 'something': [1, 2, 2, 2], 'never': [1, 2, 2, 2], 'knew': [1], 'needed': [1], 'might': [1], 'also': [1], 'whopper': [1, 1], 'burger': [1], 'king': [1, 2, 2, 2], 'perro': [1], 'negro': [1], 'bad': [1, 2], 'bunny': [1], 'feid': [1], 'mine': [1, 1, 2], 'mitski': [1], 'bridge': [1], 'use': [1, 1, 1, 1, 2, 2], 'outro': [1], 'many': [1, 1, 1, 2], 'really': [1, 2], 'gon': [2, 2], 'na': [2, 2, 2, 2], 'go': [2, 2], 'easy': [2], 'hurt': [2], 'feelings': [2], 'going': [2], 'get': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], 'chance': [2], 'six': [2, 2, 2, 2], 'minutes': [2, 2, 2, 2], 'wrong': [2], 'slim': [2, 2], 'shady': [2], 'feeling': [2], 'got': [2, 2, 2, 2, 2, 2, 2, 2, 2], 'happen': [2], 'means': [2, 2], 'think': [2, 2, 2, 2], 'trouble': [2, 2], 'big': [2, 2, 2], 'bananas': [2], 'taking': [2], 'chances': [2], 'doctor': [2], 'ordered': [2], 'beginning': [2, 2, 2], 'rap': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], 'god': [2, 2, 2, 2, 2, 2, 2, 2], 'people': [2, 2, 2, 2], 'front': [2, 2, 2, 2], 'back': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], 'nod': [2, 2, 2, 2, 2, 2], 'thinks': [2, 2], 'arms': [2, 2], 'long': [2, 2], 'enough': [2, 2, 2, 2], 'slap': [2, 2, 2, 2], 'box': [2, 2, 2, 2], 'robot': [2], 'call': [2, 2], 'rapbot': [2], 'computer': [2], 'must': [2], 'genes': [2], 'laptop': [2], 'pocket': [2], 'pen': [2], 'fat': [2], 'knot': [2], 'profit': [2], 'made': [2, 2, 2], 'livin': [2], 'killin': [2], 'ever': [2, 2], 'since': [2, 2], 'bill': [2], 'clinton': [2], 'office': [2], 'monica': [2], 'lewinsky': [2], 'feelin': [2, 2], 'nutsack': [2], 'mc': [2, 2], 'honest': [2], 'rude': [2], 'indecent': [2], 'hell': [2, 2], 'syllables': [2], 'killaholic': [2], 'kill': [2, 2], 'flippity': [2], 'wan': [2, 2], 'pissing': [2], 'match': [2], 'rappidy': [2], 'brat': [2], 'packin': [2], 'mac': [2], 'ac': [2], 'backpack': [2], 'crap': [2], 'exact': [2], 'time': [2, 2, 2, 2], 'attempt': [2], 'lyrical': [2], 'acrobat': [2], 'stunts': [2], 'practicin': [2], 'able': [2], 'break': [2], 'motherfuckin': [2, 2, 2], 'table': [2], 'couple': [2], 'faggots': [2], 'crack': [2], 'half': [2, 2], 'realized': [2], 'ironic': [2], 'signed': [2], 'aftermath': [2], 'fact': [2], 'blow': [2, 2], 'drop': [2], 'wrath': [2], 'attack': [2], 'rappers': [2, 2], 'rough': [2], 'period': [2], 'maxipad': [2], 'actually': [2], 'disastrously': [2], 'wack': [2], 'masterfully': [2], 'constructing': [2], 'masterpi√®ce': [2], 'let': [2, 2, 2], 'show': [2], 'maintaining': [2], 'ai': [2, 2], 'hard': [2, 2], 'everybody': [2, 2], 'key': [2], 'secret': [2], 'immortality': [2], 'well': [2, 2], 'truthful': [2], 'blueprint': [2], 'simply': [2], 'rage': [2], 'youthful': [2], 'exuberance': [2], 'loves': [2], 'root': [2], 'nuisance': [2], 'hit': [2], 'earth': [2], 'asteroid': [2], 'nothin': [2, 2], 'shoot': [2, 2], 'moon': [2], 'pew': [2], 'taken': [2], 'school': [2, 2, 2], 'music': [2, 2, 2], 'vehicle': [2], 'bus': [2], 'rhyme': [2, 2], 'lead': [2], 'new': [2, 2], 'full': [2, 2], 'students': [2], 'product': [2], 'rakim': [2], 'lakim': [2], 'shabazz': [2], '2pac': [2], 'cube': [2], 'hey': [2, 2, 2], 'doc': [2, 2], 'ren': [2], 'yella': [2], 'eazy': [2], 'thank': [2], 'inspired': [2], 'day': [2, 2, 2, 2], 'grow': [2], 'position': [2], 'meet': [2], 'induct': [2], 'rock': [2, 2], 'n': [2], 'roll': [2], 'hall': [2, 2], 'fame': [2, 2, 2], 'though': [2, 2, 2], 'walk': [2, 2], 'church': [2, 2], 'burst': [2], 'ball': [2], 'flames': [2, 2], 'inducted': [2], 'alcohol': [2], 'wall': [2], 'shame': [2], 'fags': [2], 'game': [2], 'flock': [2], 'plank': [2], 'tell': [2], 'fuck': [2, 2, 2, 2, 2, 2], 'thinkin': [2], 'gay': [2, 2, 2], 'lookin': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], 'boy': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], 'barely': [2], 'straight': [2, 2], 'face': [2, 2, 2], 'witnessin': [2], 'watchin': [2], 'gathering': [2], 'take': [2, 2, 2, 2, 2], 'place': [2], 'oy': [2], 'vey': [2], 'thumbs': [2], 'pat': [2], 'label': [2], 'dre': [2], 'work': [2], 'everything': [2], 'nobody': [2], 'outta': [2], 'basically': [2], 'capable': [2], 'keepin': [2], 'pace': [2], 'racin': [2], 'track': [2], 'nascar': [2, 2], 'dale': [2], 'earnhardt': [2], 'trailer': [2], 'park': [2], 'white': [2], 'trash': [2], 'kneel': [2], 'general': [2], 'zod': [2], 'planet': [2], 'krypton': [2], 'asgard': [2, 2], 'thor': [2], 'odin': [2], 'rodent': [2], 'omnipotent': [2], 'reloadin': [2], 'immediately': [2], 'bombs': [2], 'woken': [2], 'walkin': [2], 'dead': [2], 'talkin': [2], 'head': [2, 2], 'zombie': [2], 'mom': [2], 'ramen': [2], 'noodle': [2], 'common': [2], 'poodle': [2], 'doberman': [2], 'pinch': [2], 'arm': [2], 'pay': [2], 'homage': [2], 'pupil': [2], 'honesty': [2], 'brutal': [2], 'honestly': [2], 'futile': [2], 'utilize': [2], 'good': [2], 'least': [2], 'make': [2, 2, 2, 2, 2, 2, 2, 2], 'sure': [2], 'chicken': [2], 'scratch': [2], 'scribble': [2], 'doodle': [2], 'rhymes': [2, 2], 'maybe': [2], 'help': [2], 'tough': [2], 'times': [2], 'ta': [2, 2, 2], 'punchlines': [2], 'case': [2], 'unsigned': [2], 'hungry': [2, 2], 'lunchtime': [2], 'underground': [2], 'pharoahe': [2], 'monch': [2], 'grind': [2], 'crunch': [2], 'sometimes': [2], 'combine': [2], 'appeal': [2], 'skin': [2], 'color': [2], 'tryin': [2], 'censor': [2], 'line': [2, 2], 'mathers': [2], 'lp': [2], 'seven': [2], 'kids': [2], 'columbine': [2], 'put': [2, 2], 'add': [2], 'revolver': [2], 'nine': [2], 'morphin': [2], 'immortal': [2], 'comin': [2], 'portal': [2], 'stuck': [2], 'warp': [2], '2004': [2], 'pointless': [2], 'rapunzel': [2], 'fuckin': [2], 'cornrows': [2], 'write': [2], 'normal': [2, 2], 'bought': [2], 'future': [2], 'ya': [2], 'fabolous': [2], 'ray': [2, 2], 'j': [2, 2], 'mad': [2], 'fab': [2, 2], 'looked': [2], 'fag': [2], 'mayweather': [2], 'pad': [2], 'singin': [2], 'man': [2, 2, 2], 'played': [2], 'piano': [2], 'special': [2], 'cable': [2], 'channel': [2], 'went': [2], 'radio': [2], 'station': [2], 'next': [2], 'lyrics': [2], 'coming': [2], 'supersonic': [2], 'speed': [2], 'jj': [2], 'fad': [2], 'uh': [2, 2], 'sama': [2], 'lama': [2, 2], 'duma': [2], 'assumin': [2], 'human': [2], 'superhuman': [2], 'innovative': [2], 'rubber': [2], 'anything': [2], 'ricochetin': [2], 'glue': [2], 'devastating': [2], 'demonstrating': [2], 'give': [2], 'audience': [2], 'levitating': [2], 'fading': [2], 'haters': [2], 'forever': [2], 'fell': [2], 'celebrating': [2], 'motivated': [2], 'elevating': [2], 'elevator': [2], 'mainstream': [2], 'jealous': [2], 'confuse': [2], 'pop': [2], 'found': [2], 'hella': [2], 'fuse': [2], 'shock': [2], 'lose': [2, 2], 'songs': [2, 2], 'words': [2], 'occurs': [2], 'rippin': [2], 'verses': [2, 2], 'diverse': [2], 'curtains': [2], 'inadvertently': [2], 'hurtin': [2], 'murder': [2], 'prove': [2], 'nice': [2], 'sacrifice': [2], 'virgins': [2], 'flunkie': [2], 'pill': [2], 'junkie': [2], 'accolades': [2], 'skills': [2], 'brung': [2], 'bully': [2], 'mind': [2], 'leagues': [2], 'ill': [2], 'speak': [2], 'tongues': [2], 'tongue': [2], 'cheek': [2], 'drunk': [2], 'satan': [2, 2], 'fucking': [2], 'wheel': [2], 'asleep': [2], 'seat': [2], 'bumping': [2], 'heavy': [2], 'boys': [2], 'chunky': [2], 'funky': [2], 'tugging': [2], 'struggling': [2], 'angels': [2], 'fight': [2], 'devils': [2], 'askin': [2], 'eliminate': [2], 'consideration': [2], 'bitter': [2], 'hatred': [2], 'may': [2], 'patient': [2], 'sympathetic': [2], 'situation': [2], 'understand': [2], 'discrimination': [2], 'life': [2], 'handing': [2], 'lemons': [2], 'lemonade': [2], 'batter': [2], 'women': [2], 'supposed': [2], 'bake': [2], 'cake': [2], 'mistake': [2, 2], 'fatal': [2], 'overseas': [2], 'vacation': [2], 'trip': [2], 'broad': [2], 'fall': [2], 'retard': [2]})\n"
     ]
    }
   ],
   "source": [
    "inverted_index = defaultdict(list)\n",
    "\n",
    "for doc_id, doc in enumerate(preprocessed_documents):\n",
    "    for term in doc:\n",
    "        inverted_index[term].append(doc_id)\n",
    "\n",
    "print(inverted_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef070ef3-685d-45f2-9834-c310842cddf1",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c05d6f4-012e-48d1-bd7b-aeaaf1456a6b",
   "metadata": {},
   "source": [
    "## TF-IDF based retrieval model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "893866f3-4be1-4a10-ac1d-5f4efe8c22a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create a TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(\n",
    "                [\" \".join(doc) for doc in preprocessed_documents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46fbd669-f091-4644-83ed-23072cdfc425",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_documents_tfidf(query, tfidf_matrix):\n",
    "    query_vector = tfidf_vectorizer.transform([query])\n",
    "    cosine_similarities = (tfidf_matrix * query_vector.T).toarray().flatten()\n",
    "    ranked_documents = sorted(enumerate(cosine_similarities),\n",
    "                              key=lambda x: x[1], reverse=True)\n",
    "    return ranked_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d104a1d1-6a2d-4848-8aa1-da47a5c7d440",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity scores:\n",
      "\n",
      "Doc1->control.txt\t: 0.15142078194717465\n",
      "Doc2->imagination.txt\t: 0.1003258105717071\n",
      "Doc3->rap_god.txt\t: 0.015386187973523315\n"
     ]
    }
   ],
   "source": [
    "retrieved_documents_tfidf = retrieve_documents_tfidf(query, tfidf_matrix)\n",
    "print(\"Cosine Similarity scores:\\n\")\n",
    "for doc_id, score in retrieved_documents_tfidf:\n",
    "    print(f\"Doc{doc_id + 1}->{file_paths[doc_id]}\\t: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849f23c8-92d5-4368-b24c-7017191029da",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fb4241-7609-4bc4-9129-0915d720ac2a",
   "metadata": {},
   "source": [
    "## BM25 retrieval model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5707cf7-ea27-4788-bb09-4974e9aed11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# Create a BM25 index\n",
    "bm25 = BM25Okapi(preprocessed_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4dd176d7-14a4-4254-b305-8fc8fb2afa42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_documents_bm25(query, bm25, preprocessed_documents):\n",
    "    tokenized_query = preprocess(query)\n",
    "    scores = bm25.get_scores(tokenized_query)\n",
    "    ranked_documents = sorted(enumerate(scores), key=lambda x: x[1], reverse=True)\n",
    "    return ranked_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb44e876-20a5-4667-832f-298cc4bb10e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 scores:\n",
      "\n",
      "Doc1->control.txt\t: 1.1356501864548376\n",
      "Doc2->imagination.txt\t: 0.9998366301469569\n",
      "Doc3->rap_god.txt\t: 0.10877415031604586\n"
     ]
    }
   ],
   "source": [
    "retrieved_documents_bm25 = retrieve_documents_bm25(query, bm25, preprocessed_documents)\n",
    "print(\"BM25 scores:\\n\")\n",
    "for doc_id, score in retrieved_documents_bm25:\n",
    "    print(f\"Doc{doc_id + 1}->{file_paths[doc_id]}\\t: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b3f7c5-8ec2-4ac2-886e-854cad5a59e4",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994e3668-6777-4bd9-94cc-4c5218ee39f7",
   "metadata": {},
   "source": [
    "## Deep Merge based retrieval model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b289cadf-4222-4a20-9428-26853ded6e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevance scores:\n",
      "[(0, 0.15142078194717465), (1, 0.1003258105717071), (2, 0.015386187973523315)]\n",
      "[(0, 1.1356501864548376), (1, 0.9998366301469569), (2, 0.10877415031604586)]\n"
     ]
    }
   ],
   "source": [
    "sorted_bm25 = sorted(retrieved_documents_bm25, key=lambda x: x[0])\n",
    "sorted_tfidf = sorted(retrieved_documents_tfidf, key=lambda x: x[0])\n",
    "\n",
    "rel_score = [sorted_tfidf, sorted_bm25]\n",
    "print(\"Relevance scores:\")   \n",
    "for row in rel_score:\n",
    "    print(row)\n",
    " \n",
    "relevance_scores = np.array([[i[1] for i in sorted_tfidf],\n",
    "                             [i[1] for i in sorted_bm25]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b393e827-310b-4292-93d6-066eefac2223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda Scores:\n",
      "[0.69274695 0.59505676 0.06674957]\n"
     ]
    }
   ],
   "source": [
    "weights = np.array([0.45, 0.55])\n",
    "\n",
    "# Compute the lambda value using the relevance scores and weighting factors\n",
    "lambdas = np.zeros(relevance_scores.shape[1])\n",
    "for i in range(relevance_scores.shape[1]):\n",
    "    lambdas[i] = np.sum(relevance_scores[:, i] * weights) / np.sum(weights)\n",
    "\n",
    "print(f\"Lambda Scores:\\n{lambdas}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9943484c-c200-4710-87a1-3e73b4303543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.89161449 0.6546591  0.00828765]\n"
     ]
    }
   ],
   "source": [
    "# overall relevance score (for each document) using the lambda values\n",
    "overall_scores = np.sum(lambdas * relevance_scores, axis=0)\n",
    "print(overall_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb75107a-c50e-4fd0-aa33-053288c8f6a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LambdaMerged scores:\n",
      "\n",
      "Doc0->control.txt\t: 0.8916144934911177\n",
      "Doc1->imagination.txt\t: 0.6546590989198616\n",
      "Doc2->rap_god.txt\t: 0.008287648851921185\n"
     ]
    }
   ],
   "source": [
    "sorted_indices = np.argsort(overall_scores)[::-1]\n",
    "print(\"LambdaMerged scores:\\n\")\n",
    "for i in sorted_indices:\n",
    "    print(f\"Doc{i}->{file_paths[i]}\\t: {overall_scores[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37eb81dd-ed77-40b6-9a62-6adf30389f53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "info_retr",
   "language": "python",
   "name": "info_retr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
